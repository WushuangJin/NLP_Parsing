{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepress the data \n",
    "!python preprocess.py train.trees > train.trees.pre\n",
    "#run train.trees.pre through unknown.py save the output to train.trees.pre.unk\n",
    "!python unknown.py train.trees.pre > train.trees.pre.unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn a probabilistic CFG from trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tree.py < train.trees.pre.unk > grammer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve the grammer remove /n and blank line\n",
    "f = open(\"grammer.txt\",'r')\n",
    "g = open(\"new_grammer.txt\",'w')\n",
    "try:   \n",
    "    while True:        \n",
    "        line=f.readline()        \n",
    "        if len(line)==0:           \n",
    "            break        \n",
    "        if line.count('\\n')==len(line):            \n",
    "            continue        \n",
    "        g.write(line)\n",
    "finally:    \n",
    "    f.close()    \n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n",
      "PUNC -> . # 346\n"
     ]
    }
   ],
   "source": [
    "#use a dict key is every grammer and value is responding counter name \n",
    "#sort the dict and we could get the most popular grammer\n",
    "grammer_dict = {}\n",
    "file = open(\"new_grammer.txt\")\n",
    "for line in file.readlines():\n",
    "    line=line.strip('\\n')\n",
    "    if(grammer_dict.has_key(line)):\n",
    "        grammer_dict[line] = grammer_dict.get(line)+1\n",
    "    else:\n",
    "        grammer_dict[line] = 1\n",
    "print len(grammer_dict)\n",
    "sorted(grammer_dict.items(),key = lambda x:x[1])\n",
    "index = max(grammer_dict,key=grammer_dict.get)\n",
    "print index,\"#\",grammer_dict[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 752 rules in my grammer ,the most frequent rule is PUNC -> . it occurs 346 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7561\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# transfer the rules with frequency into the probability\n",
    "count_sum = 0\n",
    "for key in grammer_dict.keys():\n",
    "    count_sum = count_sum + grammer_dict.get(key)\n",
    "#count_sum is the total number of the rules\n",
    "print count_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "#grammer_prob is a grammer with probablity\n",
    "grammer_prob = copy.deepcopy(grammer_dict)\n",
    "for key in grammer_prob.keys():\n",
    "    grammer_prob[key] = log(float(grammer_prob[key])/float(count_sum),10)\n",
    "#grammer_prob is a dict whose key are rules and values are corresponding probability\\\n",
    "\n",
    "#word_search_children is a dict whose key is string after -> value is rule\n",
    "#word_search_parent is a dict whose key is rule -> value is string before ->\n",
    "#grammer_prob is a dict whose key is rule -> value is probability\n",
    "from collections import defaultdict\n",
    "word_search_children = defaultdict(list)\n",
    "for k, v in grammer_prob.items():\n",
    "    word_search_children[k.split(\"->\")[-1].strip()].append(k)\n",
    "    \n",
    "word_search_parent = defaultdict(list) \n",
    "for k,v in grammer_prob.items():\n",
    "    word_search_parent[k].append(k.split(\"->\")[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer the sentence into seperate word\n",
    "\n",
    "def to_word_list(sentence):\n",
    "    word_list = sentence.split(\" \")\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['DT'], [], [], [], [], [], [], []], [[], ['NN', 'NP_NN'], [], [], [], [], [], []], [[], [], ['MD'], [], [], [], [], []], [[], [], [], ['VB'], [], [], [], []], [[], [], [], [], ['CD'], [], [], []], [[], [], [], [], [], ['RB'], [], []], [[], [], [], [], [], [], ['NP_NN', 'NN'], []], [[], [], [], [], [], [], [], ['PUNC']]]\n",
      "[[[-3.2765192467342565], [], [], [], [], [], [], []], [[], [-2.003517974670519, -3.4014579833425564], [], [], [], [], [], []], [[], [], [-3.2765192467342565], [], [], [], [], []], [[], [], [], [-3.5775492423982374], [], [], [], []], [[], [], [], [], [-3.5775492423982374], [], [], []], [[], [], [], [], [], [-2.9754892510702753], [], []], [[], [], [], [], [], [], [-3.100427987678575, -3.4014579833425564], []], [[], [], [], [], [], [], [], [-1.3395031392694425]]]\n"
     ]
    }
   ],
   "source": [
    "#write a parsing and take the grammer and senctence as input, and output the highest_probability parse\n",
    "import copy\n",
    "def initial_table(sentence,parent_list,children_list,rule_prob):\n",
    "    l = to_word_list(sentence)\n",
    "    length = len(l)\n",
    "    table = [[[] for i in range(length)] for i in range(length)] \n",
    "    table_prob = copy.deepcopy(table)\n",
    "    for i in range(len(l)):\n",
    "        grammer_key = children_list[l[i]]\n",
    "        for word in grammer_key:\n",
    "            table_prob[i][i].append(rule_prob[word])\n",
    "            table[i][i]= table[i][i]+parent_list[word]\n",
    "    table_dict = {}\n",
    "    table_dict[\"table\"] = table\n",
    "    table_dict[\"table_prob\"] = table_prob\n",
    "    return table_dict\n",
    "result = initial_table(input_string,word_search_parent,word_search_children,grammer_prob) \n",
    "print result[\"table\"]\n",
    "print result[\"table_prob\"]\n",
    "\n",
    "#use initial_table we could get a table on the diagnol with the word of length1 and its label       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[], ['DT'], [], [], [], [], [], [], []], [[], [], ['NN', 'NP_NN'], [], [], [], [], [], []], [[], [], [], ['MD'], [], [], [], [], []], [[], [], [], [], ['VB'], [], [], [], []], [[], [], [], [], [], ['CD'], [], [], []], [[], [], [], [], [], [], ['RB'], [], []], [[], [], [], [], [], [], [], ['NP_NN', 'NN'], []], [[], [], [], [], [], [], [], [], ['PUNC']]]\n",
      "[[[], [-3.2765192467342565], [], [], [], [], [], [], []], [[], [], [-2.003517974670519, -3.4014579833425564], [], [], [], [], [], []], [[], [], [], [-3.2765192467342565], [], [], [], [], []], [[], [], [], [], [-3.5775492423982374], [], [], [], []], [[], [], [], [], [], [-3.5775492423982374], [], [], []], [[], [], [], [], [], [], [-2.9754892510702753], [], []], [[], [], [], [], [], [], [], [-3.100427987678575, -3.4014579833425564], []], [[], [], [], [], [], [], [], [], [-1.3395031392694425]]]\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#use a funciton index_table it add a column to the original table in order to change index of it\n",
    "def index_table(table):\n",
    "    \n",
    "    for l in table:\n",
    "        l.insert(0,[])\n",
    "    return table\n",
    "\n",
    "print index_table(result[\"table\"])\n",
    "print index_table(result[\"table_prob\"])\n",
    "print len(result[\"table\"])\n",
    "print len(result['table'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP', 'TOP']\n"
     ]
    }
   ],
   "source": [
    "#after all the initialization process we could implement the parse of a sentence\n",
    "from math import log\n",
    "\n",
    "def parsing_table(sentence,parent_list,children_list,rule_prob):\n",
    "    table_with_prob = initial_table(sentence,parent_list,children_list,rule_prob)\n",
    "    cky_table = index_table(table_with_prob[\"table\"])\n",
    "    cky_prob = index_table(table_with_prob[\"table_prob\"])\n",
    "    n = len(cky_table[0])\n",
    "    for width in range(2,n):\n",
    "        for start in range(0,n-width):\n",
    "            end = start+width\n",
    "            for mid in range(start+1,end):\n",
    "                list_y = cky_table[start][mid]\n",
    "                list_z = cky_table[mid][end]\n",
    "                for y in list_y:\n",
    "                    for z in list_z:\n",
    "                        temp = [y,z]\n",
    "                        child = \" \".join(temp)\n",
    "                        if(child in children_list):\n",
    "                            grammer_list=children_list[child]\n",
    "                            for grammer in grammer_list:\n",
    "                                cky_table[start][end]=cky_table[start][end]+parent_list[grammer]\n",
    "                                \n",
    "    return cky_table\n",
    "                                \n",
    "table1 = parsing_table(input_string,word_search_parent,word_search_children,grammer_prob)\n",
    "print table1[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'flight', 'should', 'be', 'eleven', 'a.m', 'tomorrow', '.']\n",
      "the flight\n"
     ]
    }
   ],
   "source": [
    "#f_string is the first line of dev.strings, we transfer it into input_string and delete \\n at the end\n",
    "#of sentence and pass it into the to_word_list transfer it into a word list\n",
    "f_string = open('dev.strings')\n",
    "input_string = f_string.readlines()[0].strip('\\n')\n",
    "l =  to_word_list(input_string)\n",
    "print l \n",
    "a = \"the\"\n",
    "b = \"flight\"\n",
    "s = [a,b]\n",
    "t = \" \".join(s)\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT -> The']\n",
      "-3.27651924673\n",
      "[['DT']]\n",
      "['NN -> flight', 'NP_NN -> flight']\n",
      "-2.00351797467\n",
      "[['NN'], ['NP_NN']]\n",
      "-3.40145798334\n",
      "[['NN'], ['NP_NN']]\n",
      "['MD -> should']\n",
      "-3.27651924673\n",
      "[['MD']]\n",
      "['VB -> be']\n",
      "-3.5775492424\n",
      "[['VB']]\n",
      "['CD -> eleven']\n",
      "-3.5775492424\n",
      "[['CD']]\n",
      "['RB -> a.m']\n",
      "-2.97548925107\n",
      "[['RB']]\n",
      "['NP_NN -> tomorrow', 'NN -> tomorrow']\n",
      "-3.10042798768\n",
      "[['NP_NN'], ['NN']]\n",
      "-3.40145798334\n",
      "[['NP_NN'], ['NN']]\n",
      "['PUNC -> .']\n",
      "-1.33950313927\n",
      "[['PUNC']]\n"
     ]
    }
   ],
   "source": [
    "#word_Search is a hashmap whose key is the value after -> value is the whole corresponding rule\n",
    "\n",
    "\n",
    "for i in range(len(l)):\n",
    "    grammer_key = word_search_children[l[i]]\n",
    "    print grammer_key\n",
    "    for i in grammer_key:\n",
    "        prob = grammer_prob[i]\n",
    "        tag = [word_search_parent[i] for i in grammer_key]\n",
    "        print prob\n",
    "        print tag\n",
    "    \n",
    "    \n",
    "                                                  \n",
    "                                                  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2.7",
   "language": "python",
   "name": "python27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
