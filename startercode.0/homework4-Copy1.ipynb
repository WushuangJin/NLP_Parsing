{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepress the data \n",
    "!python preprocess.py train.trees > train.trees.pre\n",
    "#run train.trees.pre through unknown.py save the output to train.trees.pre.unk\n",
    "!python unknown.py train.trees.pre > train.trees.pre.unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn a probabilistic CFG from trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python new_tree.py < train.trees.pre.unk > grammer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve the grammer remove /n and blank line\n",
    "f = open(\"grammer.txt\",'r')\n",
    "g = open(\"new_grammer.txt\",'w')\n",
    "try:   \n",
    "    while True:        \n",
    "        line=f.readline()        \n",
    "        if len(line)==0:           \n",
    "            break        \n",
    "        if line.count('\\n')==len(line):            \n",
    "            continue        \n",
    "        g.write(line)\n",
    "finally:    \n",
    "    f.close()    \n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP -> salt 6\n",
      "NP_NNP -> tacoma 10\n",
      "NP_PRP -> me 82\n",
      "VB -> leave 4\n",
      "SQ* -> NP PP 1\n",
      "SQ* -> NP NP 2\n",
      "FRAG_WHNP -> WHNP* PP 32\n",
      "NN -> stop 7\n",
      "NNP -> southwest 4\n",
      "TOP -> FRAG_NP_NNP PUNC 1\n",
      "S_VP -> VBP PP 2\n",
      "NP -> NP* NNS 34\n",
      "NP -> NP* NNP 30\n",
      "NNP -> u 4\n",
      "NNP -> t 4\n",
      "NNP -> w 6\n",
      "WHNP -> WRB RB 5\n",
      "NNP -> p 1\n",
      "NNP -> s 3\n",
      "NP -> NP ADJP_JJ 1\n",
      "SYM -> <unk> 3\n",
      "S_VP -> VP* INTJ_UH 2\n",
      "NNP -> k 4\n",
      "NNP -> j 4\n",
      "NP -> NN NN 9\n",
      "NNP -> d 5\n",
      "NNP -> f 6\n",
      "NNP -> a 10\n",
      "VP -> VBG NP 6\n",
      "NP -> NP* CD 17\n",
      "NP -> JJS NN 8\n",
      "S_VP -> VP* NP 73\n",
      "NN -> airfare 5\n",
      "QP* -> IN JJS 2\n",
      "PP_IN -> from 1\n",
      "POS -> 's 2\n",
      "NN -> flight 78\n",
      "VBN -> <unk> 1\n",
      "SQ -> VBZ NP 22\n",
      "NP_NNP -> atlanta 6\n",
      "PP -> TO NP 58\n",
      "FRAG_NP -> NP_NNS PP 7\n",
      "INTJ_UH -> oh 1\n",
      "NNP -> las 24\n",
      "VP* -> VP CC 7\n",
      "NP* -> NP NN 1\n",
      "FRAG_NP_NN -> <unk> 2\n",
      "NP* -> NP NP 2\n",
      "VB -> fly 5\n",
      "SQ_VP -> VBZ NP 1\n",
      "NP -> JJ NN 11\n",
      "NNP -> county 4\n",
      "ADVP -> RB PP 1\n",
      "VBZ -> is 38\n",
      "NNP -> international 2\n",
      "FRAG* -> WHNP PP 2\n",
      "VB -> serve 7\n",
      "DT -> the 128\n",
      "S_VP -> VP* PP 4\n",
      "S_VP -> VBP ADVP 1\n",
      "VBG -> arriving 12\n",
      "SQ* -> NP_NP_EX SQ* 17\n",
      "NP* -> NN CD 2\n",
      "NP -> NNP JJ 2\n",
      "NP -> NP* QP 1\n",
      "SQ_VP -> VBZ NP_NN 1\n",
      "WHNP* -> NP PP 1\n",
      "NP -> NP PP 28\n",
      "SBARQ -> SBARQ* SQ_VP 1\n",
      "DT -> that 4\n",
      "SQ_VP -> VBZ ADJP_JJR 2\n",
      "NP* -> QP NNS 1\n",
      "TOP -> FRAG_NP_NN PUNC 2\n",
      "VP* -> VB PRT_RP 1\n",
      "TOP -> SBARQ PUNC 102\n",
      "NP* -> JJS JJ 4\n",
      "NP -> NP SBAR_S_VP 1\n",
      "SQ -> VBP NP 5\n",
      "ADJP -> RBS JJ 1\n",
      "VP -> VB NP 15\n",
      "NP* -> CD NNS 1\n",
      "PP -> TO INTJ_UH 1\n",
      "NNS -> <unk> 5\n",
      "NP_NN -> today 1\n",
      "TOP -> FRAG_WHNP PUNC 33\n",
      "VP -> VBD NP 1\n",
      "VB -> like 17\n",
      "S -> INTJ_UH VP 4\n",
      "DT -> a 31\n",
      "ADJP_JJ -> nonstop 3\n",
      "NNP -> cincinnati 1\n",
      "VB -> go 2\n",
      "NP* -> NP* NN 20\n",
      "NP_NNP -> cleveland 27\n",
      "TOP -> ADJP_JJ PUNC 1\n",
      "NP_NNPS -> wednesdays 1\n",
      "NNS -> codes 2\n",
      "NN -> day 3\n",
      "ADVP_RB -> last 1\n",
      "IN -> in 49\n",
      "VP -> VBG PP 15\n",
      "CC -> and 33\n",
      "SQ_VP -> VP* PP 17\n",
      "NNP -> lake 6\n",
      "NP -> NP SBAR 13\n",
      "NP* -> NN NNP 6\n",
      "NP* -> RB DT 1\n",
      "NP_DT -> this 1\n",
      "NP* -> NP_NNS PP 44\n",
      "NP -> CD NNS 2\n",
      "JJS -> latest 4\n",
      "NP* -> NP* PP 55\n",
      "ADJP* -> RB RB 1\n",
      "VBZ -> has 1\n",
      "ADVP_RB -> here 2\n",
      "NP -> NP* SYM 2\n",
      "VB -> stop 1\n",
      "NN -> class 14\n",
      "PP -> IN NP_NNPS 1\n",
      "NP_NNP -> denver 15\n",
      "IN -> than 7\n",
      "NX -> NN NN 1\n",
      "NN -> airline 2\n",
      "NNP -> n 2\n",
      "NNP -> louis 3\n",
      "VBN -> served 2\n",
      "NP* -> CD CC 2\n",
      "FRAG -> X PP 2\n",
      "NP* -> CD CD 7\n",
      "NP_NN -> aircraft 1\n",
      "PP -> IN NP 197\n",
      "NP_NNP -> washington 8\n",
      "WHADJP -> WRB JJ 3\n",
      "NP_NNP -> boston 6\n",
      "NN -> return 4\n",
      "ADVP -> NP RB 4\n",
      "S_VP -> VB NP_PRP 2\n",
      "NP* -> NNP CC 2\n",
      "NP* -> NP_NNS VP 4\n",
      "PP* -> NP IN 1\n",
      "NNP -> orlando 1\n",
      "FRAG_NP -> NNP NN 2\n",
      "NP -> NNP NNS 1\n",
      "NP -> NNP NNP 133\n",
      "NNP -> air 3\n",
      "JJ -> round 17\n",
      "ADVP_RB -> now 2\n",
      "QP -> CC JJR 1\n",
      "SBARQ -> WHNP_WP SQ 20\n",
      "JJR -> <unk> 1\n",
      "RB -> around 3\n",
      "NN -> noon 2\n",
      "NNP -> c 6\n",
      "JJS -> shortest 3\n",
      "NN -> afternoon 8\n",
      "TOP -> FRAG_VP PUNC 8\n",
      "VP -> VP* VP 3\n",
      "VB -> be 2\n",
      "VBP -> have 7\n",
      "VB -> thank 2\n",
      "NP_NNP -> seattle 6\n",
      "VBG -> <unk> 4\n",
      "NP_NN -> lunch 3\n",
      "QP* -> CD CD 3\n",
      "CD -> hundred 8\n",
      "X_S_VP -> VP* NP 1\n",
      "NNP -> wednesday 4\n",
      "NP* -> DT CD 3\n",
      "NP -> NNP NN 12\n",
      "WHNP -> WDT NN 3\n",
      "FRAG -> PP PP 4\n",
      "S* -> INTJ_UH NP_PRP 1\n",
      "S_VP -> VB NP 17\n",
      "X -> VBZ NP_DT 1\n",
      "NP_NNP -> newark 28\n",
      "FRAG_WHNP -> WHNP PP 1\n",
      "NP_NNS -> airports 2\n",
      "IN -> before 13\n",
      "SQ_VP -> VBP ADJP_JJ 1\n",
      "ADJP -> JJR PP 2\n",
      "QP -> QP* CD 10\n",
      "FRAG_NP -> JJS NN 4\n",
      "QP* -> RB JJR 1\n",
      "NP_NNP -> montreal 8\n",
      "NNS -> stopovers 2\n",
      "VP -> VB S 2\n",
      "NP_NN -> tomorrow 6\n",
      "FRAG_NP -> NP* ADVP_RB 2\n",
      "FRAG_PP -> IN NP 4\n",
      "NP -> NP_NNP NP 9\n",
      "TOP -> X_S_VP PUNC 1\n",
      "VP_VB -> cost 3\n",
      "DT -> all 14\n",
      "PP -> TO NP_NNP 159\n",
      "NNP -> vegas 24\n",
      "CD -> eight 4\n",
      "NX_NN -> aircraft 1\n",
      "NNP -> guardia 3\n",
      "WHNP_WHNP -> WHADJP NNS 2\n",
      "VP* -> VBG PP 7\n",
      "S_VP -> MD VP 2\n",
      "NNP -> ohio 2\n",
      "NP* -> NP VP 1\n",
      "JJ -> next 6\n",
      "QP* -> RBR IN 4\n",
      "VB -> make 1\n",
      "VP -> VBP ADJP 1\n",
      "NP_NN -> flight 7\n",
      "PP* -> PP CC 1\n",
      "FRAG -> FRAG* PP 5\n",
      "NNP -> july 2\n",
      "SQ* -> NP_NN VP 1\n",
      "NN -> morning 17\n",
      "SBARQ -> WHNP_WDT SQ_VP 6\n",
      "NP_NNP -> detroit 6\n",
      "TOP -> INTJ_UH PUNC 2\n",
      "NP -> JJ NNP 3\n",
      "NP -> JJ NNS 5\n",
      "FRAG_NP -> NP* VP 3\n",
      "NP_NNP -> wednesday 11\n",
      "VP* -> VB ADVP 1\n",
      "SQ_VP -> VBP ADVP_RBS 1\n",
      "NN -> one 2\n",
      "WDT -> what 77\n",
      "VP_VBZ -> has 1\n",
      "NN -> coach 5\n",
      "ADVP_RBS -> latest 2\n",
      "NP_NNS -> stops 1\n",
      "TOP -> SQ PUNC 21\n",
      "DT -> those 1\n",
      "NNP -> jersey 4\n",
      "TOP -> FRAG_ADJP_JJS PUNC 1\n",
      "SQ -> VBP NP_NP_EX 1\n",
      "NP* -> NP_NN PP 8\n",
      "PDT -> all 3\n",
      "VP -> VP* ADVP 2\n",
      "NN -> price 11\n",
      "FRAG_NP -> JJ NNS 1\n",
      "VP -> TO VP 1\n",
      "FRAG_ADJP_JJ -> nonstop 2\n",
      "NNP -> airlines 17\n",
      "FRAG_VP -> VBG PP 1\n",
      "FRAG_NP -> NP_NNS ADJP 1\n",
      "NX -> NX* NX 1\n",
      "CD -> one 26\n",
      "IN -> on 73\n",
      "RB -> as 1\n",
      "IN -> of 39\n",
      "SQ* -> NP VP 9\n",
      "NNP -> newark 1\n",
      "FRAG_NP_NNP -> tuesday 1\n",
      "VP* -> VP* NP 2\n",
      "SQ_VP -> VBP PP 7\n",
      "ADVP_RB -> daily 4\n",
      "NNP -> san 30\n",
      "FRAG* -> NP_NNP PP 3\n",
      "FRAG_NP -> NP* NNS 3\n",
      "NN -> fare 19\n",
      "VBP -> want 3\n",
      "NN -> trip 17\n",
      "NNP -> tuesday 1\n",
      "JJS -> least 2\n",
      "PP -> IN NP_PRP 1\n",
      "S -> NP_NN VP 1\n",
      "NP -> QP NN 2\n",
      "QP* -> QP* CD 3\n",
      "S* -> ADVP_RB NP_PRP 2\n",
      "FRAG_VP -> VBG NP 1\n",
      "IN -> at 10\n",
      "NP* -> NNP CD 2\n",
      "IN -> from 223\n",
      "IN -> as 1\n",
      "ADVP_RB -> only 2\n",
      "NP* -> NP* X_JJ 1\n",
      "VP_VBN -> served 1\n",
      "FRAG -> FRAG* ADVP_RB 1\n",
      "NNP -> los 11\n",
      "NP_NNP -> orlando 14\n",
      "S_VP -> VBZ PP 3\n",
      "NP_NNS -> <unk> 1\n",
      "NP -> NN NNS 7\n",
      "NP -> NP* NP 20\n",
      "JJ -> nonstop 10\n",
      "NP* -> PDT DT 3\n",
      "NP_NNS -> wednesdays 1\n",
      "SBAR_S_VP -> TO VP_VB 1\n",
      "NNP -> la 3\n",
      "JJS -> <unk> 4\n",
      "NP -> NP* NN 46\n",
      "WHNP_WHNP -> WDT NNS 18\n",
      "NN -> type 2\n",
      "NN -> business 2\n",
      "NP_NNS -> weekdays 3\n",
      "S* -> PP NP_PRP 2\n",
      "SQ_VP -> VBP ADVP_RB 1\n",
      "VP -> VBP NP 17\n",
      "SQ -> VBZ SQ* 13\n",
      "WHADVP_WRB -> <unk> 1\n",
      "TO -> to 241\n",
      "NP -> NP VP 4\n",
      "NP* -> JJ NNP 1\n",
      "VBZ -> arrives 2\n",
      "JJ -> <unk> 13\n",
      "X -> WP IN 4\n",
      "FRAG_PP -> IN NP_NNP 1\n",
      "NNP -> cleveland 2\n",
      "NP_NP_EX -> there 20\n",
      "PP* -> IN CC 1\n",
      "RB -> a.m 8\n",
      "NP_PRP -> <unk> 2\n",
      "VBG -> leaving 13\n",
      "VBD -> <unk> 1\n",
      "NP_NNP -> baltimore 19\n",
      "VBP -> make 2\n",
      "VP* -> VBG NP 2\n",
      "FRAG* -> PP PP 1\n",
      "NP -> NP* PP 61\n",
      "VP -> VB PP 7\n",
      "NP_NNP -> westchester 1\n",
      "SQ* -> NP_PRP VP 6\n",
      "CD -> two 2\n",
      "FRAG* -> FRAG* NP 1\n",
      "NN -> way 8\n",
      "SQ* -> NP_NP_EX NP 2\n",
      "CD -> eleven 2\n",
      "VP -> VBP PP 5\n",
      "NNS -> airlines 3\n",
      "IN -> via 3\n",
      "NP_NNP -> nashville 17\n",
      "SQ_VP -> VP* VP 2\n",
      "VBP -> go 9\n",
      "VB -> <unk> 8\n",
      "NX* -> NX_NN CC 1\n",
      "VP -> VBZ NP 1\n",
      "CD -> four 3\n",
      "CD -> fifty 6\n",
      "NP_NNS -> meals 1\n",
      "NP_NNP -> miami 15\n",
      "DT -> these 11\n",
      "SQ* -> NP VP_VB 4\n",
      "FRAG -> FRAG* VP 1\n",
      "IN -> by 3\n",
      "NP_NNP -> <unk> 7\n",
      "VBP -> arrive 4\n",
      "CD -> three 2\n",
      "CD -> oh 2\n",
      "NP* -> NP* NP 1\n",
      "NP_NNP -> pittsburgh 12\n",
      "NP_NNP -> oakland 7\n",
      "VB -> show 89\n",
      "VP -> VB NP_NN 7\n",
      "RBR -> less 4\n",
      "S -> NP_NN VP_VBN 1\n",
      "NN -> time 2\n",
      "VBP -> do 9\n",
      "NP* -> CC NNP 1\n",
      "FRAG -> NP INTJ_UH 1\n",
      "NP_DT -> these 8\n",
      "CD -> ten 11\n",
      "NNP -> jose 7\n",
      "VB -> arrive 4\n",
      "NN -> tomorrow 3\n",
      "NP* -> NP CC 15\n",
      "FRAG_NP -> NP* NP_NN 2\n",
      "ADVP_RB -> first 1\n",
      "FRAG_NP -> NP PP 6\n",
      "NP_NN -> noon 6\n",
      "FRAG* -> FRAG* PP 2\n",
      "VP* -> VP* PP 14\n",
      "WHNP_WDT -> that 21\n",
      "FRAG -> PP VP 1\n",
      "S_VP -> VB NP_NNS 2\n",
      "NP_NNS -> fares 3\n",
      "NN -> cost 2\n",
      "NP* -> NNP NNP 28\n",
      "VP* -> VBG NP_NNP 1\n",
      "SQ_VP -> VBP NP_NNS 1\n",
      "SQ_VP -> VBP NP_NNP 1\n",
      "NP_DT -> those 4\n",
      "NP_NNP -> july 3\n",
      "NNP -> francisco 15\n",
      "NP_CD -> one 1\n",
      "VP -> VBN PP 3\n",
      "VP -> MD VP 19\n",
      "IN -> around 6\n",
      "IN -> about 6\n",
      "NP_NNP -> thursday 4\n",
      "MD -> <unk> 1\n",
      "RB -> apart 4\n",
      "NNP -> new 25\n",
      "FRAG -> ADVP_RB NP_NNP 1\n",
      "FRAG_NP -> NP* NN 3\n",
      "DT -> <unk> 2\n",
      "NP* -> CD NN 2\n",
      "RB -> <unk> 5\n",
      "NN -> reservation 2\n",
      "S_VP -> VBP NP 2\n",
      "MD -> would 9\n",
      "FRAG_NP -> NP* NP 3\n",
      "PP -> PP* S_VP_VBG 1\n",
      "FRAG -> WHNP PP 1\n",
      "NP_NNP -> dallas 14\n",
      "NNS -> hours 4\n",
      "CD -> thirty 11\n",
      "NP_NNP -> philadelphia 8\n",
      "NNS -> numbers 3\n",
      "TOP -> FRAG_PP PUNC 5\n",
      "NNS -> dollars 4\n",
      "NP* -> NP_NNP PP 1\n",
      "INTJ_UH -> please 5\n",
      "NP -> NP_NNS VP 2\n",
      "VBP -> <unk> 8\n",
      "WHNP -> WHNP PP 3\n",
      "NP -> DT JJ 5\n",
      "X -> WRB IN 1\n",
      "VP -> VBZ PP 2\n",
      "NNS -> ones 2\n",
      "S -> ADVP_RB VP 2\n",
      "NN -> dinner 6\n",
      "CD -> <unk> 7\n",
      "VP* -> VB NP_PRP 82\n",
      "FRAG_NP -> DT NNS 1\n",
      "PP -> IN ADJP_JJ 1\n",
      "SQ* -> NP_DT NP 1\n",
      "FRAG_NP -> NP* NP_NNS 1\n",
      "NP -> NP_DT PP 2\n",
      "PP* -> PP* IN 1\n",
      "VBG -> departing 5\n",
      "TOP -> X_SBARQ PUNC 1\n",
      "FRAG_NP -> NN NNS 1\n",
      "VBZ -> <unk> 2\n",
      "NNP -> friday 3\n",
      "NP_NNP -> monday 5\n",
      "WHNP_WP -> what 20\n",
      "ADJP -> JJ PP 1\n",
      "NN -> nonstop 2\n",
      "VB -> visit 2\n",
      "NP -> NP_CD PP 1\n",
      "NNS -> minutes 5\n",
      "S* -> NP_PRP ADVP_RB 1\n",
      "S_VP_VBG -> arriving 1\n",
      "VP* -> VP* X_TO 1\n",
      "NP* -> NP NP_NN 2\n",
      "CD -> twenty 2\n",
      "NN -> number 5\n",
      "NP_NNP -> memphis 14\n",
      "ADJP -> ADJP* PP 1\n",
      "NP_NN -> airline 2\n",
      "NP* -> NP PP 81\n",
      "CC -> <unk> 1\n",
      "SQ_VP -> VBZ ADVP_RBS 1\n",
      "PP -> IN NP_DT 11\n",
      "FRAG_NP -> NP* SBAR 1\n",
      "VP -> VB NP_NNS 1\n",
      "NP -> QP RB 3\n",
      "PUNC -> . 346\n",
      "VP -> VB NP_NNP 2\n",
      "NN -> night 2\n",
      "NP* -> NP SBAR 1\n",
      "ADJP_JJ -> last 3\n",
      "WRB -> how 9\n",
      "JJ -> other 2\n",
      "PUNC -> ? 123\n",
      "NN -> today 1\n",
      "NN -> leg 2\n",
      "NP_NNP -> cincinnati 9\n",
      "VP* -> VBP PP 14\n",
      "NN -> aircraft 2\n",
      "S_VP -> VBZ NP 1\n",
      "SYM -> d 1\n",
      "CD -> twelve 5\n",
      "NP -> NP* VP 8\n",
      "NP_NNP -> tampa 14\n",
      "NNP -> diego 8\n",
      "CD -> six 11\n",
      "NP_NNP -> chicago 9\n",
      "NNP -> delta 1\n",
      "NP -> CD CD 2\n",
      "SQ_VP -> VBP NP 4\n",
      "NP_NNP -> burbank 6\n",
      "SYM -> p 1\n",
      "FRAG_NP -> NP* PP 50\n",
      "IN -> with 9\n",
      "IN -> after 16\n",
      "NNP -> long 3\n",
      "WHNP -> WHNP ADJP_JJ 1\n",
      "X_JJ -> nonstop 1\n",
      "VP -> VP* NP_NN 1\n",
      "PP -> IN NP_NN 9\n",
      "NNS -> stops 4\n",
      "MD -> 'd 10\n",
      "NNS -> fares 15\n",
      "NNP -> beach 3\n",
      "RB -> much 5\n",
      "INTJ_UH -> thanks 2\n",
      "NP* -> JJ NN 8\n",
      "NNP -> westchester 4\n",
      "INTJ_UH -> okay 2\n",
      "NP_DT -> any 2\n",
      "TOP -> FRAG_ADJP_JJ PUNC 2\n",
      "INTJ_UH -> <unk> 2\n",
      "NNP -> tampa 1\n",
      "SQ_VP -> VBP PP_IN 1\n",
      "NP -> NP NP 2\n",
      "NP* -> DT NN 19\n",
      "SQ* -> PP SQ* 2\n",
      "JJS -> earliest 3\n",
      "NP* -> NP_NN CC 1\n",
      "VP -> VBP FRAG 1\n",
      "PP -> IN NP_NNS 3\n",
      "PP -> IN NP_NNP 239\n",
      "WHNP -> WHADJP NNS 1\n",
      "NP -> DT NX 1\n",
      "TOP -> S PUNC 54\n",
      "NP -> QP NNS 7\n",
      "S -> NP_PRP VP 36\n",
      "NP -> DT NN 70\n",
      "WP -> what 4\n",
      "TOP -> S_VP PUNC 98\n",
      "ADJP_JJR -> better 2\n",
      "NP -> NP* ADVP 3\n",
      "PP -> PP* NP 1\n",
      "NNS -> meals 4\n",
      "VBZ -> serves 2\n",
      "SQ* -> MD SQ* 1\n",
      "NNP -> sunday 2\n",
      "MD -> should 4\n",
      "FRAG* -> NP NP 2\n",
      "VBZ -> lives 2\n",
      "FRAG_ADJP_JJS -> cheapest 1\n",
      "VP -> VBP S_VP 8\n",
      "TOP -> FRAG_NP PUNC 105\n",
      "SBAR -> WHNP_WDT S 1\n",
      "SQ* -> VBZ SQ* 1\n",
      "NP -> DT NNS 59\n",
      "VP* -> VBP RB 1\n",
      "NP -> NP NNS 1\n",
      "VB -> travel 3\n",
      "NN -> transportation 7\n",
      "RB -> o'clock 5\n",
      "NN -> <unk> 16\n",
      "SBARQ -> WHNP_WHNP SQ 19\n",
      "NP -> CD NN 10\n",
      "S_VP -> VBZ ADVP_RB 1\n",
      "NP_NNP -> dulles 1\n",
      "FRAG_VP -> VP* PP 6\n",
      "NNP -> thursday 6\n",
      "FRAG -> X NP_NNP 1\n",
      "NNP -> paul 2\n",
      "VBP -> serve 5\n",
      "S -> S* S 2\n",
      "JJS -> cheapest 16\n",
      "NP* -> DT JJ 17\n",
      "S_VP -> VP* VP 3\n",
      "VBP -> depart 7\n",
      "NNP -> milwaukee 1\n",
      "S_VP -> TO VP 21\n",
      "NNP -> petersburg 3\n",
      "SQ -> MD SQ* 4\n",
      "SBARQ -> WHNP SQ_VP 42\n",
      "NP_NN -> dinner 14\n",
      "NP_NNP -> indianapolis 11\n",
      "SBARQ -> WHNP SQ 8\n",
      "QP -> RB CD 3\n",
      "NP -> NP* RB 7\n",
      "MD -> can 2\n",
      "NP -> WDT NNS 1\n",
      "FRAG* -> NP_NN PP 1\n",
      "FRAG -> NP_NNP PP 5\n",
      "NN -> ground 8\n",
      "RB -> p.m 22\n",
      "PRT_RP -> <unk> 1\n",
      "FRAG_NP -> NP* CD 1\n",
      "SQ* -> NP_PRP VP_VB 1\n",
      "S -> NP VP 8\n",
      "S -> NP_NNP VP_VBZ 1\n",
      "SBARQ -> WHNP_WDT SQ 5\n",
      "DT -> any 2\n",
      "WHNP* -> WHNP* PP 25\n",
      "NP* -> NP* CC 1\n",
      "FRAG_NP -> JJS NNS 1\n",
      "NNP -> angeles 11\n",
      "VB -> return 2\n",
      "NP -> NP_NNP NP_NNP 5\n",
      "NNS -> flights 156\n",
      "NN -> code 2\n",
      "VP_VB -> stop 1\n",
      "VBP -> stop 4\n",
      "NP* -> JJ JJ 1\n",
      "TOP -> SBAR PUNC 1\n",
      "NP_NN -> price 8\n",
      "NP_NNP -> ontario 6\n",
      "NP_NNP -> american 2\n",
      "NNP -> american 10\n",
      "NP* -> NP* VP 1\n",
      "VP* -> VB PP 13\n",
      "X_SBARQ -> WHNP SQ_VP 1\n",
      "IN -> between 13\n",
      "VBP -> are 27\n",
      "SQ* -> PP PP 17\n",
      "NP* -> CD RB 4\n",
      "CD -> seven 11\n",
      "JJ -> last 3\n",
      "VP* -> VBZ PP 2\n",
      "VB -> list 7\n",
      "NNP -> dulles 1\n",
      "NNP -> united 6\n",
      "FRAG -> X NP 2\n",
      "NP_NNP -> minneapolis 10\n",
      "NP -> NP* SBAR 6\n",
      "FRAG -> FRAG* NP 2\n",
      "SQ* -> NP ADJP_JJ 1\n",
      "NP_NNP -> friday 9\n",
      "SQ_VP -> VBZ ADJP_JJ 3\n",
      "VB -> find 2\n",
      "SQ -> INTJ_UH SQ* 1\n",
      "JJ -> many 3\n",
      "DT -> this 5\n",
      "VP_VB -> leave 1\n",
      "TOP -> NP PUNC 3\n",
      "NP -> NP_NNS PP 3\n",
      "NNP -> city 29\n",
      "NP* -> NP* JJ 2\n",
      "CD -> nine 7\n",
      "NNP -> kansas 14\n",
      "NN -> evening 11\n",
      "VP -> VB S_VP 13\n",
      "NP_NNP -> milwaukee 19\n",
      "VP -> VP* PP 16\n",
      "JJR -> less 3\n",
      "NNP -> monday 5\n",
      "SBARQ* -> PP WHNP_WHNP 1\n",
      "SQ* -> NP SQ* 2\n",
      "NP -> NP_NN PP 1\n",
      "QP* -> QP* IN 1\n",
      "VBZ -> does 7\n",
      "FRAG_NP -> NP_NN PP 6\n",
      "FRAG_NP -> CD NNS 1\n",
      "NP_PRP -> you 6\n",
      "NP* -> NP* SYM 4\n",
      "NNP -> <unk> 4\n",
      "VBZ -> leaves 4\n",
      "FRAG -> NP NP 2\n",
      "NP* -> NP* NNP 15\n",
      "VP_VBG -> <unk> 1\n",
      "S -> S* VP 6\n",
      "S* -> S CC 2\n",
      "NP* -> NP* CD 32\n",
      "FRAG_NP -> NN NN 3\n",
      "NP_PRP -> it 2\n",
      "NP_NNP -> toronto 10\n",
      "SYM -> a 1\n",
      "FRAG_NP -> NP VP_VBG 1\n",
      "IN -> <unk> 6\n",
      "NP_NNP -> tuesday 4\n",
      "SQ_VP -> VBP VP 1\n",
      "RBS -> least 1\n",
      "JJ -> first 12\n",
      "ADVP -> ADVP_RB PP 2\n",
      "NP -> CD RB 21\n",
      "CC -> or 7\n",
      "WHNP* -> WHNP PP 31\n",
      "NP_NNP -> houston 16\n",
      "VP -> VP* ADJP 1\n",
      "VP -> VP* NP 6\n",
      "NP* -> DT JJS 11\n",
      "TOP -> FRAG PUNC 28\n",
      "SQ -> X SQ* 1\n",
      "PP -> PP* PP 1\n",
      "WDT -> <unk> 1\n",
      "NP_NNP -> saturday 7\n",
      "X_TO -> to 2\n",
      "NP_NNP -> charlotte 9\n",
      "SQ_VP -> VP* NP 1\n",
      "NNP -> saint 8\n",
      "ADVP_RB -> <unk> 4\n",
      "NP_NNP -> sunday 8\n",
      "NNP -> ontario 1\n",
      "S_VP -> VBZ NP_NN 1\n",
      "SBARQ -> WHADVP_WRB SQ 1\n",
      "SQ -> VBP SQ* 26\n",
      "NP* -> NP_NNP CC 6\n",
      "VB -> explain 3\n",
      "NP* -> DT ADJP 1\n",
      "VBP -> fly 2\n",
      "FRAG_NP -> NP* ADJP 1\n",
      "NP_NNP -> columbus 10\n",
      "NP* -> NP* X_TO 1\n",
      "VBP -> need 16\n",
      "NP_NNS -> flights 57\n",
      "NP -> NP* NP_NNP 4\n",
      "SBAR -> WHNP SQ 1\n",
      "NP -> NNP POS 2\n",
      "VP* -> VB ADVP_RB 1\n",
      "WHNP -> WDT NNS 61\n",
      "NP_PRP -> i 44\n",
      "CD -> five 18\n",
      "SBAR -> WHNP_WDT S_VP 20\n",
      "ADJP_JJ -> <unk> 3\n",
      "S_VP -> VBP NP_NN 2\n",
      "SQ -> VBZ NP_PRP 2\n",
      "NP -> NP* NP_NN 2\n",
      "NP* -> NP* ADVP_RB 2\n",
      "WHNP -> WHNP_WDT PP 17\n",
      "VP_VB -> <unk> 1\n",
      "VB -> have 5\n",
      "NP* -> NN NN 8\n",
      "NP* -> JJS NN 2\n",
      "NN -> lunch 2\n",
      "NP* -> DT NNP 7\n",
      "NNP -> york 21\n",
      "WHNP_WDT -> which 28\n",
      "SQ_VP -> VBP NP_NN 4\n",
      "IN -> for 5\n",
      "VP -> VBG NP_NNP 1\n",
      "NP_NNP -> phoenix 19\n",
      "CD -> forty 2\n",
      "VP* -> VBP NP_NNP 6\n",
      "NP_NN -> <unk> 3\n",
      "VBP -> leave 11\n",
      "WDT -> which 5\n",
      "NP_NNP -> delta 1\n",
      "NNP -> saturday 1\n"
     ]
    }
   ],
   "source": [
    "# use a dict key is every grammer and value is responding counter name \n",
    "#sort the dict and we could get the most popular grammer\n",
    "grammer_dict = {}\n",
    "file = open(\"new_grammer.txt\")\n",
    "for line in file.readlines():\n",
    "    line=line.strip('\\n')\n",
    "    if len(line.split())==3:\n",
    "        line = ' '.join(line.split()[:2])+' '+line.split()[2].lower()\n",
    "    if(grammer_dict.has_key(line)):\n",
    "        grammer_dict[line] = grammer_dict.get(line)+1\n",
    "    else:\n",
    "        grammer_dict[line] = 1\n",
    "sorted(grammer_dict.items(),key = lambda x:x[1])\n",
    "index = max(grammer_dict,key=grammer_dict.get)\n",
    "for v,k in grammer_dict.items():\n",
    "    print v,k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 752 rules in my grammer ,the most frequent rule is PUNC -> . it occurs 346 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import defaultdict\n",
    "# transfer the rules with frequency into the probability\n",
    "count_sum = 0\n",
    "for key in grammer_dict.keys():\n",
    "    count_sum = count_sum + grammer_dict.get(key)\n",
    "#count_sum is the total number of the rules\n",
    "grammer_freq=defaultdict(int)\n",
    "for key,value in grammer_dict.items():\n",
    "    k = key.split(' -> ')[0]\n",
    "    grammer_freq[k]+=value\n",
    "grammer_final={}\n",
    "for key,value in grammer_dict.items():\n",
    "    k = key.split(' -> ')[0]\n",
    "    grammer_final[key]=value*1.0/grammer_freq[k] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer the sentence into seperate word\n",
    "\n",
    "def to_word_list(sentence):\n",
    "    word_list = sentence.split(\" \")\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'flight', 'should', 'be', 'eleven', 'a.m', 'tomorrow', '.']\n",
      "['the', 'flight', 'should', 'be', 'eleven', 'a.m', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "#f_string is the first line of dev.strings, we transfer it into input_string and delete \\n at the end\n",
    "#of sentence and pass it into the to_word_list transfer it into a word list\n",
    "f_string = open('dev.strings')\n",
    "input_string = f_string.readlines()[0].strip('\\n')\n",
    "l =  to_word_list(input_string)\n",
    "l0 = []\n",
    "for word in l:\n",
    "    l0.append(word.lower())\n",
    "print l\n",
    "print l0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grammer_prob is a dict whose key are rules and values are corresponding probability\\\n",
    "#word_search_children is a dict whose key is string after -> value is rule\n",
    "#word_search_parent is a dict whose key is rule -> value is string before ->\n",
    "#grammer_prob is a dict whose key is rule -> value is probability\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "import re\n",
    "#grammer_prob is a grammer with probablity\n",
    "grammer_prob = copy.deepcopy(grammer_dict)\n",
    "\n",
    "for key in grammer_dict.keys():\n",
    "    grammer_prob[key] = log(grammer_final[key],10)\n",
    "\n",
    "word_search_children = defaultdict(list)\n",
    "for k, v in grammer_prob.items():\n",
    "    word_search_children[k.split(\"->\")[-1].strip()].append(k)\n",
    "\n",
    "\n",
    "word_search_parent = defaultdict(list) \n",
    "for k,v in grammer_prob.items():\n",
    "    word_search_parent[k].append(k.split(\"->\")[0].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('DT', -0.18945522061366268)], [], [], [], [], [], [], []]\n",
      "[[], [('NN', -0.5566117172145995), ('NP_NN', -0.8624721360836793)], [], [], [], [], [], []]\n",
      "[[], [], [('MD', -0.8129133566428555)], [], [], [], [], []]\n",
      "[[], [], [], [('VB', -1.919078092376074)], [], [], [], []]\n",
      "[[], [], [], [], [('CD', -1.845098040014257)], [], [], []]\n",
      "[[], [], [], [], [], [('RB', -0.8211858826088454)], [], []]\n",
      "[[], [], [], [], [], [], [('NP_NN', -0.9294189257142926), ('NN', -1.9715850651854174)], []]\n",
      "[[], [], [], [], [], [], [], [('PUNC', -0.1320967439223066)]]\n"
     ]
    }
   ],
   "source": [
    "#write a parsing and take the grammer and senctence as input, and output the highest_probability parse\n",
    "# import copy\n",
    "def initial_table(sentence,children_list,rule_prob):\n",
    "    l0 = []\n",
    "    l = to_word_list(sentence)\n",
    "    for word in l:\n",
    "        l0.append(word.lower())\n",
    "    length = len(l0)\n",
    "    table = [[[] for i in range(length)] for i in range(length)] \n",
    "    for i in range(len(l0)):\n",
    "        if(l0[i] in children_list.keys()):\n",
    "            grammer_key = children_list[l0[i]]\n",
    "            for word in grammer_key:\n",
    "                table[i][i].append( (word_search_parent[word][0],rule_prob[word]) )\n",
    "        else:\n",
    "            grammer_key = children_list[\"<unk>\"]\n",
    "            for word in grammer_key:\n",
    "                table[i][i].append( (word_search_parent[word][0],rule_prob[word]) )\n",
    "#     table_dict = {}\n",
    "#     table_dict[\"table\"] = table\n",
    "#     table_dict[\"table_prob\"] = table_prob\n",
    "    return table\n",
    "result = initial_table(input_string,word_search_children,grammer_prob) \n",
    "for line in result:\n",
    "    print line\n",
    "\n",
    "#use initial_table we could get a table on the diagnol with the word of length1 and its label       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [('DT', -0.18945522061366268)], [], [], [], [], [], [], []]\n",
      "[[], [], [('NN', -0.5566117172145995), ('NP_NN', -0.8624721360836793)], [], [], [], [], [], []]\n",
      "[[], [], [], [('MD', -0.8129133566428555)], [], [], [], [], []]\n",
      "[[], [], [], [], [('VB', -1.919078092376074)], [], [], [], []]\n",
      "[[], [], [], [], [], [('CD', -1.845098040014257)], [], [], []]\n",
      "[[], [], [], [], [], [], [('RB', -0.8211858826088454)], [], []]\n",
      "[[], [], [], [], [], [], [], [('NP_NN', -0.9294189257142926), ('NN', -1.9715850651854174)], []]\n",
      "[[], [], [], [], [], [], [], [], [('PUNC', -0.1320967439223066)]]\n"
     ]
    }
   ],
   "source": [
    "#use a funciton index_table it add a column to the original table in order to change index of it\n",
    "def index_table(table):\n",
    "    for l in table:\n",
    "        l.insert(0,[])\n",
    "    return table\n",
    "\n",
    "for line in index_table(result):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TOP', -16.223114671461772, 0, 7, 8, 13, 0, 7)\n"
     ]
    }
   ],
   "source": [
    "#after all the initialization process we could implement the parse of a sentence\n",
    "from math import log\n",
    "\n",
    "def argmax_top(table):\n",
    "    top =  table[0][-1]\n",
    "    if top == []:\n",
    "        return []\n",
    "    if len(top) ==1:\n",
    "        return top[0]\n",
    "    max = 0\n",
    "    for i in range(1,len(top)):\n",
    "        if(top[i][1]>top[max][1]):\n",
    "            max = i\n",
    "    return top[max]\n",
    "\n",
    "def parsing_table(sentence,children_list,grammer_prob):\n",
    "    table_with_prob = initial_table(sentence,children_list,grammer_prob)\n",
    "    cky_table = index_table(table_with_prob)\n",
    "    n = len(cky_table[0])\n",
    "    for width in range(2,n):\n",
    "        for start in range(0,n-width):\n",
    "            end = start+width\n",
    "            for mid in range(start+1,end):\n",
    "                list_y = cky_table[start][mid]\n",
    "                list_z = cky_table[mid][end]\n",
    "                index_w = 0\n",
    "                for index_y,y in enumerate(list_y):\n",
    "                    for index_z,z in enumerate(list_z):\n",
    "                        tag_y = y[0]\n",
    "                        prob_y = y[1]\n",
    "                        tag_z = z[0]\n",
    "                        prob_z = z[1]\n",
    "                        tag_x = tag_y + \" \" + tag_z\n",
    "                        if(tag_x in children_list):\n",
    "                            x_yz = children_list[tag_x]\n",
    "                            for w in x_yz:\n",
    "                                new_tag = word_search_parent[w][0]\n",
    "                                prob_x = prob_y + prob_z + grammer_prob[w]\n",
    "                                cky_table[start][end].append((new_tag,prob_x,start,mid,end,index_y,index_z,index_w))\n",
    "                                index_w = index_w + 1\n",
    "# index_z is the index of element in table[mid][end] table[mid][end] is a list \n",
    "# index_y is the index of element in table[start][mid],table [mid][end] is a list                          \n",
    "    return cky_table\n",
    "                                \n",
    "table1 = parsing_table(input_string,word_search_children,grammer_prob)\n",
    "best_top = argmax_top(table1)\n",
    "print best_top\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP (S (NP (DT The) (NN flight)) (VP (MD should) (VP (VB be) (NP (NP* (CD eleven) (RB a.m)) (NN tomorrow))))) (PUNC .))\n"
     ]
    }
   ],
   "source": [
    "#use backtrack method to get route with the highest probability\n",
    "def backtrack(cky_table,i,j,k,sentence):\n",
    "    if(j == i+1):\n",
    "        return \"(\"+cky_table[i][j][k][0] + \" \"+ sentence[i]+\")\"\n",
    "    item = cky_table[i][j][k]\n",
    "    new_tag=item[0]\n",
    "    start1,end1 = item[2:4]\n",
    "    start2,end2 = item[3:5]\n",
    "    k1 = item[5]\n",
    "    k2 = item[6]\n",
    "    return '('+new_tag +' '+backtrack(cky_table,start1,end1,k1,sentence)+' '+backtrack(cky_table,start2,end2,k2,sentence)+')'   \n",
    "q2_result = backtrack(table1,best_top[2],best_top[4],best_top[-1],l)\n",
    "print q2_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem2 answer : the output of parser on the first line of dev.strings is above and \n",
    "the corresponding log probability is -17.745297989080456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flight should be eleven a.m tomorrow .\n",
      "('TOP', -16.223114671461772, 0, 7, 8, 13, 0, 7)\n"
     ]
    }
   ],
   "source": [
    "f_test = open('dev.strings')\n",
    "input_test = f_test.readlines()[0].strip('\\n')\n",
    "print input_test\n",
    "table_test = parsing_table(input_test,word_search_children,grammer_prob)\n",
    "print argmax_top(table_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import time\n",
    "\n",
    "sentence_length = []\n",
    "parsing_time = []\n",
    "file_p3 = open(\"dev.strings\",'r')\n",
    "for line in file_p3:\n",
    "    line = line.strip('\\n')\n",
    "    word_list_q3 = to_word_list(line)\n",
    "    length_q3 = len(word_list_q3)\n",
    "    start_time = time.clock()\n",
    "    table_q3 = parsing_table(line,word_search_children,grammer_prob)\n",
    "    best_top_q3 = argmax_top(table_q3)\n",
    "    if(not best_top_q3 == []):\n",
    "        q3_result  = backtrack(table_q3,best_top_q3[2],best_top_q3[4],best_top_q3[-1],word_list_q3)\n",
    "    else:\n",
    "        continue\n",
    "    interval =time.clock() - start_time\n",
    "    sentence_length.append(log(length_q3,10))\n",
    "    parsing_time.append(log(interval,10))\n",
    "print len(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.09909634 -5.76856963]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#using sentence length x axis using parsing time as y axis \n",
    "#make a plot \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_length_q3 = np.array(sentence_length)\n",
    "y_time_q3 = np.array(parsing_time)\n",
    "\n",
    "f1 = np.polyfit(x_length_q3,y_time_q3,1)\n",
    "y_values = np.polyval(f1,x_length_q3)\n",
    "print f1\n",
    "\n",
    "plt.figure\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.scatter(sentence_length,parsing_time,label = \"scatter plot\")\n",
    "plt.plot(x_length_q3,y_values,label = \"ployfit curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer to Q3: we plot the scatter plot of parsing time(y axis) versus sentence length(x axis),and we ployfit the scatter plot for a functional curve with log-scale. the first parameter is nearly 3 it seems a curve y = x^3 transfered into a log-form for both side , the reason is that when we implement the parsing who has 3 for loops\n",
    "( width, start, mid ) ,so the time complexity is O(n^3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the answer of parser output to the q4_result.txt\n",
    "with open(\"dev.strings\",'r') as f_q4_read:\n",
    "    with open(\"q4_result_test.txt\",'w') as f_q4_write:\n",
    "        for line in f_q4_read:\n",
    "            line = line.strip('\\n')\n",
    "            word_list_q4 = to_word_list(line)\n",
    "            table_q4 = parsing_table(line,word_search_children,grammer_prob)\n",
    "            best_top_q4 = argmax_top(table_q4)\n",
    "            if(not best_top_q4 == []):\n",
    "                q4_result  = backtrack(table_q4,best_top_q4[2],best_top_q4[4],best_top_q4[-1],word_list_q4)\n",
    "            else:\n",
    "                q4_result  = \"\"\n",
    "            f_q4_write.write(q4_result+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python postprocess.py q4_result_test.txt > q4_result_test_final.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q4_result_test_final.txt\t440 brackets\n",
      "dev.trees\t474 brackets\n",
      "matching\t405 brackets\n",
      "precision\t0.920454545455\n",
      "recall\t0.854430379747\n",
      "F1\t0.886214442013\n"
     ]
    }
   ],
   "source": [
    "!python evalb.py q4_result_test_final.txt dev.trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem5 method3: Change the input word into lowercase, For example in the grammer word ,all and All should be treated as one word, So I change the input sentence and the word in grammar into lowercase,then apply the pcfg to it the f1 score has been improved to 0.886\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.219725055987\n",
      "-0.219725162691\n"
     ]
    }
   ],
   "source": [
    "# first improvememt is using smoothing method to \n",
    "# for v,k in grammer_dict.items():\n",
    "#     print v,k\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "new_dict_freq = copy.deepcopy(grammer_dict)\n",
    "for key in new_dict_freq.keys():\n",
    "    new_dict_freq[key] = new_dict_freq[key]+0.001\n",
    "    \n",
    "grammer_freq_q5=defaultdict(int)\n",
    "for key,value in new_dict_freq.items():\n",
    "    k = key.split(' -> ')[0]\n",
    "    grammer_freq_q5[k]+=value\n",
    "    \n",
    "grammer_final_q5={}\n",
    "for key,value in new_dict_freq.items():\n",
    "    k = key.split(' -> ')[0]\n",
    "    grammer_final_q5[key]=value*1.0/grammer_freq_q5[k] \n",
    "# for v, k in grammer_final.items():\n",
    "#     print v,k\n",
    "# for v, k in grammer_final_q5:\n",
    "#     print v,k\n",
    "\n",
    "grammer_prob_q5 = copy.deepcopy(grammer_dict)\n",
    "\n",
    "for key in grammer_dict.keys():\n",
    "    grammer_prob_q5[key] = log(grammer_final_q5[key],10)\n",
    "\n",
    "print grammer_prob[\"NP_PRP -> me\"]\n",
    "print grammer_prob_q5[\"NP_PRP -> me\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dev.strings\",'r') as f_q4_read:\n",
    "    with open(\"q5_result.txt\",'w') as f_q4_write:\n",
    "        for line in f_q4_read:\n",
    "            line = line.strip('\\n')\n",
    "            word_list_q4 = to_word_list(line)\n",
    "            table_q4 = parsing_table(line,word_search_children,grammer_prob_q5)\n",
    "            best_top_q4 = argmax_top(table_q4)\n",
    "            if(not best_top_q4 == []):\n",
    "                q4_result  = backtrack(table_q4,best_top_q4[2],best_top_q4[4],best_top_q4[-1],word_list_q4)\n",
    "            else:\n",
    "                q4_result  = \"\"\n",
    "            f_q4_write.write(q4_result+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q5_improvement1\t436 brackets\n",
      "dev.trees\t474 brackets\n",
      "matching\t400 brackets\n",
      "precision\t0.917431192661\n",
      "recall\t0.84388185654\n",
      "F1\t0.879120879121\n"
     ]
    }
   ],
   "source": [
    "!python postprocess.py q5_result.txt > q5_improvement1\n",
    "!python evalb.py q5_improvement1 dev.trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem5: method 1: Using Add-One smoothing for the training data, I add 1 to all counts and recompute the probabilty, However it did not help for improving my parser, the reason is that add 1 smoothing method smooths too much "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dev2.strings\",'r') as f_q4_read:\n",
    "    with open(\"q5_result2.txt\",'w') as f_q4_write:\n",
    "        for line in f_q4_read:\n",
    "            line = line.strip('\\n')\n",
    "            word_list_q4 = to_word_list(line)\n",
    "            table_q4 = parsing_table(line,word_search_children,grammer_prob_q5)\n",
    "            best_top_q4 = argmax_top(table_q4)\n",
    "            if(not best_top_q4 == []):\n",
    "                q4_result  = backtrack(table_q4,best_top_q4[2],best_top_q4[4],best_top_q4[-1],word_list_q4)\n",
    "            else:\n",
    "                q4_result  = \"\"\n",
    "            f_q4_write.write(q4_result+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q5_improvement2\t187 brackets\n",
      "dev2.trees\t264 brackets\n",
      "matching\t18 brackets\n",
      "precision\t0.096256684492\n",
      "recall\t0.0681818181818\n",
      "F1\t0.079822616408\n"
     ]
    }
   ],
   "source": [
    "!python postprocess.py q5_result2.txt > q5_improvement2\n",
    "!python evalb.py q5_improvement2 dev2.trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem5 method2, As I used add-one smoothing method,but it seemd smoothing too much for the given data. So as a result, I consider using add-lambda smoothing(lambda we choose here as 0.001).and result are above.However it have no help for improving the F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.parses.post\t435 brackets\n",
      "dev.trees\t474 brackets\n",
      "matching\t400 brackets\n",
      "precision\t0.919540229885\n",
      "recall\t0.84388185654\n",
      "F1\t0.880088008801\n"
     ]
    }
   ],
   "source": [
    "!python rbranch.py -i dev.strings -g new_grammer.txt -o test_rbranch.txt\n",
    "!python postprocess.py test_rbranch.txt > dev.parses.post\n",
    "!python evalb.py dev.parses.post dev.trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess.py < train.trees | python unknown.py > tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
